{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 1 --driver-memory 2g pyspark-shell --master yarn --deploy-mode cluster'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-4.newprolab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab04_demenev</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fadb3826c88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[2]\") \\\n",
    "                    .appName(\"lab04_demenev\") \\\n",
    "                    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import datetime\n",
    "def print_worktime(t):\n",
    "    h = int(t//3600)\n",
    "    m = int(t // 60)\n",
    "    s = (t % 60)\n",
    "    print('Код отработал за {0} часов {1} минут {2} секунд'.format(h,m,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2022-01-06 18:46 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = \"\"\"{\"visits\": [{\"url\": \"http://sweetrading.ru/?p=900\", \"timestamp\": 1419717886224}, {\"url\": \"http://sweetrading.ru/?p=884\", \"timestamp\": 1419717884437}, {\"url\": \"http://sweetrading.ru/?p=1002\", \"timestamp\": 1419717816375}, {\"url\": \"http://101.ru/?an=port_channel_mp3\", \"timestamp\": 1419717804934}, {\"url\": \"http://sweetrading.ru/?cat=62\", \"timestamp\": 1419714194423}, {\"url\": \"http://sweetrading.ru/?p=1046\", \"timestamp\": 1419713998481}, {\"url\": \"http://sweetrading.ru/?p=978\", \"timestamp\": 1419713927085}, {\"url\": \"http://sweetrading.ru/?cat=171\", \"timestamp\": 1419713908863}, {\"url\": \"http://sweetrading.ru/?cat=62\", \"timestamp\": 1419713908679}, {\"url\": \"http://sweetrading.ru/?p=3648\", \"timestamp\": 1419713798879}, {\"url\": \"http://oesex.ru/955457\", \"timestamp\": 1419595564407}, {\"url\": \"http://www.interfax.ru/russia/408800\", \"timestamp\": 1419542965224}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=30\", \"timestamp\": 1418818241900}, {\"url\": \"http://www.interfax.ru/russia/413508\", \"timestamp\": 1418802080857}, {\"url\": \"http://www.euroavtoprokat.ru/sitemap/car-rental/france.htm\", \"timestamp\": 1418722961181}, {\"url\": \"http://www.euroavtoprokat.ru/sitemap/car-rental.htm\", \"timestamp\": 1418722945825}, {\"url\": \"http://www.euroavtoprokat.ru/car-rental/germany.htm\", \"timestamp\": 1418722937847}, {\"url\": \"http://www.euroavtoprokat.ru/car-rental/germany.htm\", \"timestamp\": 1418722923196}, {\"url\": \"http://www.euroavtoprokat.ru/sitemap/car-rental.htm\", \"timestamp\": 1418722909804}, {\"url\": \"http://www.eavtoprokat.ru/prokat-avto/france\", \"timestamp\": 1418646101953}, {\"url\": \"http://www.wordparts.ru/numeral/\", \"timestamp\": 1418592793587}, {\"url\": \"http://rsdn.ru/forum/alg/3305190.flat\", \"timestamp\": 1418591162814}, {\"url\": \"http://www.euroavtoprokat.ru/car-rental/turkey/istanbul.htm\", \"timestamp\": 1418571531780}, {\"url\": \"http://citieslist.ru/\", \"timestamp\": 1418488992092}, {\"url\": \"http://www.euroavtoprokat.ru/car-rental/turkey/istanbul.htm\", \"timestamp\": 1418480798674}, {\"url\": \"http://rutv.ru/brand/show/episode/453757\", \"timestamp\": 1418253037406}, {\"url\": \"http://www.fodors.com/community/europe/best-car-rental-company-in-italy.cfm\", \"timestamp\": 1418247198586}, {\"url\": \"http://wheelsabroad.com/car-rental/united-kingdom/england/london?gclid=cjwkeaia-5-kbrdylpg5096r8masjabqedm4cmiichc-_-ewkbtsqyci5bu9ucwvjmxp4o0tficaarocljdw_wcb\", \"timestamp\": 1418245144696}, {\"url\": \"http://lestinet.com/site/stopagent.ru\", \"timestamp\": 1418243376170}, {\"url\": \"http://android-help.ru/q2a/16774/\\u043a\\u0430\\u043a-\\u043f\\u043e\\u043b\\u0443\\u0447\\u0438\\u0442\\u044c-root-\\u043f\\u0440\\u0430\\u0432\\u0430-\\u043d\\u0430-philips-w832-android-4-0-4\", \"timestamp\": 1418169606439}, {\"url\": \"http://club.dns-shop.ru/rabinovich/blog/\\u044f-\\u0432\\u0441\\u0435-\\u0435\\u0449\\u0435-\\u0434\\u0435\\u0440\\u0436\\u0443\\u0441\\u044c-\\u043e\\u0431\\u0437\\u043e\\u0440-\\u0441\\u043c\\u0430\\u0440\\u0442\\u0444\\u043e\\u043d\\u0430-philips-xenium-w832/\", \"timestamp\": 1418169602505}, {\"url\": \"http://www.supportforum.philips.com/ru/showthread.php?1529-philips-xenium-w832/page6\", \"timestamp\": 1418167859617}, {\"url\": \"http://www.supportforum.philips.com/ru/showthread.php?842-\\u043d\\u0435-\\u0440\\u0430\\u0431\\u043e\\u0442\\u0430\\u0435\\u0442-gps-\\u0432-\\u0441\\u043c\\u0430\\u0440\\u0442\\u0444\\u043e\\u043d\\u0435-philips-xenium-w832\", \"timestamp\": 1418166430112}, {\"url\": \"http://rabota.ua/info/jobsearcher/post/umora.aspx\", \"timestamp\": 1418114698621}, {\"url\": \"http://www.enter.ru/product/appliances/myasorubka-philips-hr2728-2020103007131\", \"timestamp\": 1418053557067}, {\"url\": \"http://www.ferra.ru/ru/byt/news/2013/12/02/polaris-pmg-1805/\", \"timestamp\": 1417866883735}, {\"url\": \"http://www.ferra.ru/ru/byt/news/2013/10/12/bosch-mfw6-propower/\", \"timestamp\": 1417862586856}, {\"url\": \"http://www.linotype.com/1266/neuehelvetica-family.html\", \"timestamp\": 1417856979616}, {\"url\": \"http://www.linotype.com/1546/tradegothic-family.html?site=webfonts\", \"timestamp\": 1417812010753}, {\"url\": \"http://www.vandelaydesign.com/best-ecommerce-website-designs/\", \"timestamp\": 1417807232287}, {\"url\": \"http://www.awwwards.com/20-of-the-very-best-e-commerce-web-sites.html\", \"timestamp\": 1417805189928}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=82\", \"timestamp\": 1417711286305}, {\"url\": \"http://www.just.ru/myasorubki/56658_elektromyasorybky_kenwood_mg_450/?from=yandex_msk&utm_source=yandex&utm_medium=cpc&utm_campaign=10817239_model_bytovaya-tehnika-melkaya_msk_p_api&utm_content=612422293_2792852770_\\u043c\\u044f\\u0441\\u043e\\u0440\\u0443\\u0431\\u043a\\u0443 mg 450&position_type=premi\", \"timestamp\": 1417701042306}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=5\", \"timestamp\": 1417695760398}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=5\", \"timestamp\": 1417689964129}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=17\", \"timestamp\": 1417683034834}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417608945879}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=24\", \"timestamp\": 1417605700777}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=24\", \"timestamp\": 1417605639264}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=82\", \"timestamp\": 1417605624817}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg470-meat-grinder-0wmg470008\", \"timestamp\": 1417604804579}, {\"url\": \"http://livedemo00.template-help.com/magento_48517/blackberry-bold-9000-phone.html\", \"timestamp\": 1417604730951}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg470-meat-grinder-0wmg470008\", \"timestamp\": 1417548651645}, {\"url\": \"http://www.kenwoodworld.com/en-int/products/blenders/meat-grinders/mg474-meat-grinder\", \"timestamp\": 1417548321763}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417548310507}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417548309162}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001?feat=6405fda1-43cc-42cc-8860-1c2a492555c5&tabsegment=key-features\", \"timestamp\": 1417548297576}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001?tabsegment=key-features\", \"timestamp\": 1417548284970}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417548264964}, {\"url\": \"http://www.kenwoodworld.com/en-int/products/blenders/meat-grinders\", \"timestamp\": 1417546314287}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg700-meat-grinder-0wmg700006\", \"timestamp\": 1417545459520}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg700-meat-grinder-0wmg700006\", \"timestamp\": 1417545200191}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/kmix-by-kenwood/kmix-kitchen-machines-/kmx51-kmix-kitchen-machine-0wkmx51002\", \"timestamp\": 1417545116313}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/---mg517---0wmg517007\", \"timestamp\": 1417544991760}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417544967371}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001?feat=ac86d868-3ea4-4523-93e1-885bbf4222cd&tabsegment=key-features\", \"timestamp\": 1417544772661}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001?feat=3a288c22-e5f2-448e-a573-ccde95fd2341&tabsegment=key-features\", \"timestamp\": 1417544765049}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001?feat=ac86d868-3ea4-4523-93e1-885bbf4222cd&tabsegment=key-features\", \"timestamp\": 1417544748628}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001?tabsegment=key-features\", \"timestamp\": 1417544731238}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417544522237}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417544351791}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/-mg350-0w21910001\", \"timestamp\": 1417544282950}, {\"url\": \"http://www.kenwoodworld.com/ru-ru\", \"timestamp\": 1417544269909}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg516-meat-grinder-and-roto-food-cutter-0wmg516006?tabsegment=specifications\", \"timestamp\": 1417544204394}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg516-meat-grinder-and-roto-food-cutter-0wmg516006\", \"timestamp\": 1417544190747}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg516-meat-grinder-and-roto-food-cutter-0wmg516006\", \"timestamp\": 1417544045014}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg516-meat-grinder-and-roto-food-cutter-0wmg516006?tabsegment=specifications\", \"timestamp\": 1417544035023}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg516-meat-grinder-and-roto-food-cutter-0wmg516006\", \"timestamp\": 1417544015196}, {\"url\": \"http://www.kenwoodworld.com/ru-ru\", \"timestamp\": 1417544004579}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009?tabsegment=specifications\", \"timestamp\": 1417543914820}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543814629}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543642699}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543628088}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543616074}, {\"url\": \"http://www.kenwoodworld.com/uk/products/food-mixers/chef-major-attachments/potato-peeler-at444-awat444001\", \"timestamp\": 1417543439173}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543352117}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543294005}, {\"url\": \"http://www.kenwoodworld.com/uk/search-results\", \"timestamp\": 1417543192107}, {\"url\": \"http://www.kenwoodworld.com/uk\", \"timestamp\": 1417543022466}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009?tabsegment=specifications\", \"timestamp\": 1417542940415}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009?tabsegment=support\", \"timestamp\": 1417542907491}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009?tabsegment=specifications\", \"timestamp\": 1417542866623}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009\", \"timestamp\": 1417542858206}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009\", \"timestamp\": 1417542839578}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009\", \"timestamp\": 1417542795850}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009\", \"timestamp\": 1417542742883}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009\", \"timestamp\": 1417542725367}, {\"url\": \"http://www.kenwoodworld.com/ru-ru/all-products/blenders-mixers-and-meat-grinders/meat-grinders-ru/mg510-meat-grinder-0wmg510009\", \"timestamp\": 1417542659966}, {\"url\": \"http://www.kenwoodworld.com/ru-ru\", \"timestamp\": 1417542501523}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=24\", \"timestamp\": 1417542435930}, {\"url\": \"http://www.shop-script.ru/platform/\", \"timestamp\": 1417473193974}, {\"url\": \"http://101.ru/?an=port_channel_mp3&channel=34\", \"timestamp\": 1417451297674}]} \"\"\"\n",
    "visits_test = \"[http://metanol.lv/news/dakota_nort_khochet_poekhat_v_swc/, http://metanol.lv/news/, http://metanol.lv/news/sbornaja_avstralii_nakonec_to_podala_sostav_na_pervyj_raund_kubka_luchshikh_par/, http://metanol.lv/news/, http://metanol.lv/news/sparring_betard_sparta_vroclav_zks_row_rybnik_57_33/, http://metanol.lv/news/, http://deita.ru/news/auto/18.03.2015/4860769-honda-priostanovila-postavku-lyubimogo-u-primortsev-avtomobilya/, http://primorye.ru/, http://deita.ru/news/culture/13.03.2015/4857537-pevitsa-linda-priznalas-v-lyubvi-k-vladivostoku-i-rasskazala-pro-novye-proekty/, http://speedway-press.ru/2015/03/, http://speedway-press.ru/, http://speedway-press.ru/, http://irgiz.narod.ru/, http://speedway-press.ru/2015/03/, http://speedway-press.ru/, http://speedway-press.ru/, http://irgiz.narod.ru/, http://irgiz.narod.ru/, http://irgiz.narod.ru/, http://irgiz.narod.ru/, http://n52.adshostnet.com/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://vestiprim.ru/2015/03/19/, http://primorye.ru/, http://vestiprim.ru/2015/03/19/, http://primorye.ru/] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "domain = urlparse(test_dict).netloc\n",
    "print(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metanol.lv',\n",
       " 'metanol.lv',\n",
       " 'metanol.lv',\n",
       " 'metanol.lv',\n",
       " 'metanol.lv',\n",
       " 'metanol.lv',\n",
       " 'deita.ru',\n",
       " 'primorye.ru',\n",
       " 'deita.ru',\n",
       " 'speedway-press.ru']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'(https?://([^\\s]+)/)'\n",
    "list_of_url = re.findall(pattern, visits_test)[:10]\n",
    "[urlparse(url[0]).netloc for url in list_of_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"timestamp\": 1419717886224', '1419717886224'),\n",
       " ('\"timestamp\": 1419717884437', '1419717884437'),\n",
       " ('\"timestamp\": 1419717816375', '1419717816375'),\n",
       " ('\"timestamp\": 1419717804934', '1419717804934'),\n",
       " ('\"timestamp\": 1419714194423', '1419714194423'),\n",
       " ('\"timestamp\": 1419713998481', '1419713998481'),\n",
       " ('\"timestamp\": 1419713927085', '1419713927085'),\n",
       " ('\"timestamp\": 1419713908863', '1419713908863'),\n",
       " ('\"timestamp\": 1419713908679', '1419713908679'),\n",
       " ('\"timestamp\": 1419713798879', '1419713798879')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_t = r'(\"timestamp\": (\\d+))'\n",
    "list_of_t = re.findall(pattern_t, test_dict)[:10]\n",
    "list_of_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(ArrayType(StringType()))\n",
    "def parse_visits(visits: str):\n",
    "    list_of_url = re.findall(pattern, visits)\n",
    "    return [urlparse(url[0]).netloc for url in list_of_url]\n",
    "#     return \".\".join([url[1] for url in list_of_url]).split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.udf(DoubleType())\n",
    "def parse_length(times: str):\n",
    "    list_of_t = re.findall(pattern_t, times)\n",
    "    list_of_t = [float(t[1]) for t in list_of_t]\n",
    "    length = max(list_of_t) - min(list_of_t)\n",
    "    return round(length,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visits_test.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gender', 'string'),\n",
       " ('age', 'string'),\n",
       " ('uid', 'string'),\n",
       " ('user_json', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn('visits', parse_visits('user_json'))\n",
    "train_df = train_df.withColumn('length', parse_length('user_json'))\n",
    "# train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+-------------+\n",
      "|gender|  age|                 uid|           user_json|              visits|       length|\n",
      "+------+-----+--------------------+--------------------+--------------------+-------------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|[zebra-zoya.ru, n...|6.978153933E9|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|[sweetrading.ru, ...| 2.26658855E9|\n",
      "+------+-----+--------------------+--------------------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gender', 'string'),\n",
       " ('age', 'string'),\n",
       " ('uid', 'string'),\n",
       " ('user_json', 'string'),\n",
       " ('visits', 'array<string>'),\n",
       " ('length', 'double')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_f = f.udf(lambda a, b: str(a) + '_' + str(b), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn(\"gender_age\", concat_f('gender','age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12154212650104526"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.filter((f.col('age') == \"-\") | (f.col('gender') == \"-\")).count() / train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.filter((f.col('age') != \"-\") | (f.col('gender') != \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# другой подход\n",
    "# enc_age = {'18-24' : 1,\n",
    "#           '25-34' : 2,\n",
    "#           '35-44' : 3,\n",
    "#           '45-54' : 4,\n",
    "#           '>=55' : 5}\n",
    "# enc_gender = {'F': 0,\n",
    "#              'M': 1}\n",
    "\n",
    "# age_enc = f.udf(lambda x: enc_age[x], ByteType())\n",
    "# gender_enc = f.udf(lambda x: enc_gender[x], ByteType())\n",
    "\n",
    "# train_df = train_df.withColumn('age_enc', age_enc('age'))\n",
    "# train_df = train_df.withColumn('gender_enc', gender_enc('gender'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_map = {'M_18-24' : 1,\n",
    "          'M_25-34' : 2,\n",
    "          'M_35-44' : 3,\n",
    "          'M_45-54' : 4,\n",
    "          'M_>=55' : 5,\n",
    "          'F_18-24' : 6,\n",
    "          'F_25-34' : 7,\n",
    "          'F_35-44' : 8,\n",
    "          'F_45-54' : 9,\n",
    "          'F_>=55' : 10,\n",
    "          }\n",
    "label_enc = f.udf(lambda x: enc_map[x], ByteType())\n",
    "train_data = train_df.withColumn('labels', label_enc('gender_age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+-------------+----------+\n",
      "|gender|  age|                 uid|           user_json|              visits|       length|gender_age|\n",
      "+------+-----+--------------------+--------------------+--------------------+-------------+----------+\n",
      "|     F|18-24|d50192e5-c44e-4ae...|{\"visits\": [{\"url...|[zebra-zoya.ru, n...|6.978153933E9|   F_18-24|\n",
      "|     M|25-34|d502331d-621e-472...|{\"visits\": [{\"url...|[sweetrading.ru, ...| 2.26658855E9|   M_25-34|\n",
      "+------+-----+--------------------+--------------------+--------------------+-------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'age', 'uid', 'user_json', 'visits', 'length', 'gender_age']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.select('uid', 'visits', 'labels')\n",
    "\n",
    "# другой подход\n",
    "# train_data = train_df.select('uid', 'visits', 'age_enc', 'gender_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"visits\", outputCol=\"visits_enc\")\n",
    "rf = RandomForestClassifier(featuresCol='visits_enc', labelCol=\"labels\", numTrees=50)\n",
    "# vecAssembler = VectorAssembler(outputCol=\"features\") # , handleInvalid=\"keep\"\n",
    "# vecAssembler.setInputCols([\"visits_enc\", \"length\"])\n",
    "# # другой подход\n",
    "# rf = RandomForestClassifier(featuresCol='visits_enc', labelCol=\"age_enc\", numTrees=15)\n",
    "\n",
    "# gbt = GBTClassifier(featuresCol='visits_enc', labelCol=\"gender_enc\", maxBins=50, stepSize=0.1, maxDepth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(inputCol=\"visits\", outputCol=\"visits_enc\")\n",
    "# model_cv = cv.fit(train_data)\n",
    "# result = model_cv.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "            cv,\n",
    "            rf\n",
    "        ])\n",
    "\n",
    "# другой подход\n",
    "# cv = CountVectorizer(inputCol=\"visits\", outputCol=\"visits_enc\")\n",
    "# model_cv = cv.fit(train_data)\n",
    "# train_data = model_cv.transform(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data.select('uid', 'age_enc', 'gender_enc', 'visits_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy', labelCol = \"labels\", predictionCol=\"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= train_data.randomSplit([0.8, 0.2])\n",
    "# train = train_data.sampleBy(\"labels\", fractions={1: 0.8,\n",
    "#                                                  2: 0.8,\n",
    "#                                                  3: 0.8,\n",
    "#                                                  4: 0.8,\n",
    "#                                                  5: 0.8,\n",
    "#                                                  6: 0.8,\n",
    "#                                                  7: 0.8,\n",
    "#                                                  8: 0.8,\n",
    "#                                                  9: 0.8,\n",
    "#                                                  10: 0.8}, seed=42).cache()\n",
    "# test = train_data.join(train, on=\"uid\", how=\"leftanti\").coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'uid',\n",
       " 'user_json',\n",
       " 'visits',\n",
       " 'length',\n",
       " 'gender_age',\n",
       " 'labels']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|gender_age|count(uid)|\n",
      "+----------+----------+\n",
      "|    M_>=55|       624|\n",
      "|    F_>=55|       720|\n",
      "|   M_18-24|      1624|\n",
      "|   M_45-54|      1741|\n",
      "|   F_45-54|      2061|\n",
      "|   F_18-24|      2340|\n",
      "|   F_35-44|      3426|\n",
      "|   M_35-44|      4095|\n",
      "|   F_25-34|      5474|\n",
      "|   M_25-34|      6962|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupby('gender_age').agg(f.count('uid')).sort('count(uid)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29067"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_f = train.filter((f.col('gender_age') == \"M_>=55\") | (f.col('gender_age') == \"F_>=55\")).select(\"*\").sample(fraction=(1124/28968)*3)\n",
    "# train = train.union(sample_f).cache()\n",
    "# train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.cache()\n",
    "train = train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+--------------------+--------------------+-------------+----------+------+\n",
      "|gender|  age|                 uid|           user_json|              visits|       length|gender_age|labels|\n",
      "+------+-----+--------------------+--------------------+--------------------+-------------+----------+------+\n",
      "|     F|18-24|0500be5a-e84b-4ad...|{\"visits\": [{\"url...|[www.roddoma.ru, ...|  8.7331155E7|   F_18-24|     6|\n",
      "|     F|18-24|052d1f2d-f12f-435...|{\"visits\": [{\"url...|[www.gorenskoe-sp...|          0.0|   F_18-24|     6|\n",
      "|     F|18-24|05474d68-66b2-4a9...|{\"visits\": [{\"url...|[www.xnxx.com, ww...|     101006.0|   F_18-24|     6|\n",
      "|     F|18-24|0554173d-0976-4b9...|{\"visits\": [{\"url...|[online.translate...|2.158560583E9|   F_18-24|     6|\n",
      "|     F|18-24|05647555-f621-4d2...|{\"visits\": [{\"url...|[www.startsmile.r...| 8.97816291E9|   F_18-24|     6|\n",
      "|     F|18-24|056983f8-f953-494...|{\"visits\": [{\"url...|[evo-centr.e-stil...| 3.46205001E8|   F_18-24|     6|\n",
      "|     F|18-24|057ec46a-ca88-403...|{\"visits\": [{\"url...|[go.mail.ru, domn...|8.758790257E9|   F_18-24|     6|\n",
      "|     F|18-24|058ade4f-279b-4e8...|{\"visits\": [{\"url...|[mail.rambler.ru,...|      76001.0|   F_18-24|     6|\n",
      "|     F|18-24|058ec29e-9d95-471...|{\"visits\": [{\"url...|[exir.ru, www.aif...|   1.731037E9|   F_18-24|     6|\n",
      "|     F|18-24|05adadec-9786-415...|{\"visits\": [{\"url...|[film4ik.ru, go.m...|1.746161521E9|   F_18-24|     6|\n",
      "|     F|18-24|05b3a3f2-5034-406...|{\"visits\": [{\"url...|[www.gpsmap.su, w...|    3520000.0|   F_18-24|     6|\n",
      "|     F|18-24|05d82ca1-616e-40e...|{\"visits\": [{\"url...|[urbanus.ru, urba...|9.033240617E9|   F_18-24|     6|\n",
      "|     F|18-24|061330ed-958b-479...|{\"visits\": [{\"url...|[www.avito.ru, ww...|    4.92056E8|   F_18-24|     6|\n",
      "|     F|18-24|061408f8-8e2e-408...|{\"visits\": [{\"url...|[ckopo.net, ckopo...|9.391939811E9|   F_18-24|     6|\n",
      "|     F|18-24|0616a65a-b0dd-496...|{\"visits\": [{\"url...|[bigcinema.tv, bi...|    4.33016E8|   F_18-24|     6|\n",
      "|     F|18-24|06220bee-b34d-4a8...|{\"visits\": [{\"url...|[psoriaz.jimdo.co...|   4.895914E9|   F_18-24|     6|\n",
      "|     F|18-24|06443f51-6d05-4a3...|{\"visits\": [{\"url...|[www.spektakli-on...| 4.44533001E8|   F_18-24|     6|\n",
      "|     F|18-24|06575113-ef5d-4a1...|{\"visits\": [{\"url...|[www.mixmir.net, ...|          1.0|   F_18-24|     6|\n",
      "|     F|18-24|06607fe8-6d46-453...|{\"visits\": [{\"url...|[pass.rzd.ru, pay...| 4.16036001E8|   F_18-24|     6|\n",
      "|     F|18-24|09161442-ae41-40c...|{\"visits\": [{\"url...|[play.mob.ua, sof...|7.299080551E9|   F_18-24|     6|\n",
      "+------+-----+--------------------+--------------------+--------------------+-------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Код отработал за 0 часов 4 минут 6.415687799453735 секунд\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "\n",
    "# другой подход\n",
    "# rf_model_age = rf.fit(train)\n",
    "# rf_model_age = gbt.fit(train)\n",
    "print_worktime(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M_18-24',\n",
       " 'M_25-34',\n",
       " 'M_35-44',\n",
       " 'M_45-54',\n",
       " 'M_>=55',\n",
       " 'F_18-24',\n",
       " 'F_25-34',\n",
       " 'F_35-44',\n",
       " 'F_45-54',\n",
       " 'F_>=55']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enc_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2\n",
    "list(enc_map.keys())[x-1].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_g = f.udf(lambda x: list(enc_map.keys())[int(x)-1].split(\"_\")[0])\n",
    "decode_age = f.udf(lambda x: list(enc_map.keys())[int(x)-1].split(\"_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.select(decode_age('prediction').alias('age'), decode_g('prediction').alias('gender'), 'uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+\n",
      "|  age|gender|                 uid|\n",
      "+-----+------+--------------------+\n",
      "|25-34|     M|0513b3f0-4ada-4ee...|\n",
      "|25-34|     M|053b77f9-9c8c-467...|\n",
      "|25-34|     M|05f08c3f-04d7-4da...|\n",
      "|25-34|     M|06041b7c-ca15-4c4...|\n",
      "|25-34|     M|065a265a-db0c-489...|\n",
      "|25-34|     M|093ee658-beef-4d6...|\n",
      "|25-34|     M|0940cdad-7a11-42b...|\n",
      "|25-34|     M|09801ac3-0cc7-40b...|\n",
      "|25-34|     M|0a2438b3-1181-420...|\n",
      "|25-34|     M|0a38a1d1-fec6-4d2...|\n",
      "|25-34|     M|0a410c43-0552-494...|\n",
      "|25-34|     M|0a6868f4-2cda-425...|\n",
      "|25-34|     M|0a8636d3-db74-447...|\n",
      "|25-34|     M|192d666b-5ff7-473...|\n",
      "|25-34|     M|194079e6-ef4b-4ad...|\n",
      "|25-34|     M|19eff2e6-a8c9-418...|\n",
      "|25-34|     M|1f150051-3415-4db...|\n",
      "|25-34|     M|1f217bd3-e7f9-481...|\n",
      "|25-34|     M|1f45548c-d8db-403...|\n",
      "|25-34|     M|1f936078-f8d8-4dc...|\n",
      "+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2412671475038891"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.save('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Stopped {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтение\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_aleksandr.demenev\",\n",
    "    \"failOnDataLoss\": 'False',\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "parsed_sdf = kafka_sdf.select(\n",
    "    json_tuple(col(\"value\").cast(\"string\"), 'uid', 'visits').alias('uid', 'visits')\n",
    ")\n",
    "\n",
    "parsed_sdf = parsed_sdf.withColumn('visits', parse_visits('visits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = PipelineModel.load('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_sdf.isStreaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 items\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 02:03 streaming/chk/chk_kafka/offsets/0\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:41 streaming/chk/chk_kafka/offsets/1\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/10\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/11\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/12\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/13\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/14\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/15\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/16\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/17\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/18\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/19\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/2\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/20\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/21\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/22\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/23\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/24\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/25\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/26\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/27\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/28\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/29\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/3\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/30\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/31\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/32\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/33\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/34\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/35\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/36\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/37\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/38\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/39\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/4\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/5\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/6\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/7\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/8\r\n",
      "-rw-r--r--   3 aleksandr.demenev aleksandr.demenev        446 2023-02-25 13:42 streaming/chk/chk_kafka/offsets/9\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls streaming/chk/chk_kafka/offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kill_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.transform(parsed_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.select(decode_age('prediction').alias('age'), decode_g('prediction').alias('gender'), 'uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4418.start.\n: java.lang.IllegalStateException: Cannot start query with id baf764d0-dc74-4c9b-81fb-2b51bf6c997c as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:345)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-29d5d8eb687b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"aleksandr.demenev\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mwrite_kafka_params\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"streaming/chk/chk_kafka\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4418.start.\n: java.lang.IllegalStateException: Cannot start query with id baf764d0-dc74-4c9b-81fb-2b51bf6c997c as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:345)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# запись\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"aleksandr.demenev\"\n",
    "}\n",
    "predictions.select(f.to_json(f.struct(*predictions.columns)).alias('value')).writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-475ba6ad1ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkafka_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1305\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1306\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'stop'"
     ]
    }
   ],
   "source": [
    "kafka_sdf.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Queries with streaming sources must be executed with writeStream.start();;\\nkafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o992.collectToPython.\n: org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nkafka\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:389)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\n\tat org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:36)\n\tat org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:52)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:61)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:67)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:69)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3365)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-bba2b58c6608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkafka_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Queries with streaming sources must be executed with writeStream.start();;\\nkafka'"
     ]
    }
   ],
   "source": [
    "kafka_sdf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
